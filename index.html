<!DOCTYPE html>
<html lang="en" class="scroll-smooth">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Diabetic Retinopathy Screening Report</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <style>
        .chart-container {
            position: relative;
            width: 100%;
            max-width: 600px;
            margin-left: auto;
            margin-right: auto;
            height: 300px;
            max-height: 400px;
        }
        @media (min-width: 768px) {
            .chart-container {
                height: 350px;
            }
        }
        .tab-button.active {
            border-color: #d97706;
            color: #d97706;
            font-weight: 600;
        }
        .tab-content {
            display: none;
        }
        .tab-content.active {
            display: block;
        }
    </style>
    <!-- Chosen Palette: Warm Neutrals (Stone, White, Amber) -->
    <!-- Application Structure Plan: The SPA mimics the structure of the provided Word template (Abstract, Intro, etc.) using a sticky top navigation bar for scrolling. This is intuitive for the user who is familiar with the template. The "Results & Discussion" section is the interactive core, using tabs to separate the analyses requested in the screenshot: (1) Descriptive Stats, (2) Categorical Plots, (3) Explanatory Numerical Plots. This structure directly addresses the user's questions, solves the "categorical vs. numerical data" conflict by separating the analyses, and presents the findings in a clear, digestible dashboard format. -->
    <!-- Visualization & Content Choices:
        - Report Info: CSV 'Status' column (Categorical).
        - Goal: Show distribution of 200-sample subset.
        - Viz/Presentation: Table (Stats), Bar Chart (Viz), Pie Chart (Viz).
        - Interaction: Charts are rendered dynamically on load, with tooltips on hover.
        - Justification: These are the standard and correct visualizations for categorical frequency data.
        - Library/Method: Chart.js (Canvas).
        - Report Info: Screenshot questions about numerical plots (Histogram, Scatter, Correlation).
        - Goal: Explain *what* these plots are for, since the data isn't available.
        - Viz/Presentation: HTML/Tailwind-styled text blocks with simple div-based placeholder diagrams.
        - Interaction: Static explanation.
        - Justification: Directly answers the user's question while clarifying *why* it can't be run on the provided CSV.
        - Report Info: PDF content (Intro, Methods, ML Vision).
        - Goal: Fulfill the remaining report sections.
        - Viz/Presentation: Formatted text in cards, using lists and paragraphs.
        - Interaction: Static text.
        - Justification: Directly ports the source paper's content into the report structure.
        - Library/Method: HTML/Tailwind.
    -->
    <!-- CONFIRMATION: NO SVG graphics used. NO Mermaid JS used. -->
</head>
<body class="bg-stone-50 font-sans text-stone-800 leading-relaxed">

    <header class="sticky top-0 z-50 bg-white/90 backdrop-blur-sm shadow-sm">
        <nav class="container mx-auto px-4 sm:px-6 lg:px-8">
            <div class="flex justify-between items-center h-16">
                <div class="flex-shrink-0">
                    <span class="text-xl font-bold text-amber-600">DR Screening Report</span>
                </div>
                <div class="hidden sm:block sm:ml-6">
                    <div class="flex space-x-4">
                        <a href="#abstract" class="text-stone-600 hover:text-amber-600 px-3 py-2 rounded-md text-sm font-medium">Abstract</a>
                        <a href="#introduction" class="text-stone-600 hover:text-amber-600 px-3 py-2 rounded-md text-sm font-medium">Introduction</a>
                        <a href="#methods" class="text-stone-600 hover:text-amber-600 px-3 py-2 rounded-md text-sm font-medium">Materials & Methods</a>
                        <a href="#results" class="text-stone-600 hover:text-amber-600 px-3 py-2 rounded-md text-sm font-medium">Results & Discussion</a>
                        <a href="#conclusion" class="text-stone-600 hover:text-amber-600 px-3 py-2 rounded-md text-sm font-medium">Conclusion</a>
                        <a href="#references" class="text-stone-600 hover:text-amber-600 px-3 py-2 rounded-md text-sm font-medium">References</a>
                    </div>
                </div>
            </div>
        </nav>
    </header>

    <main class="container mx-auto px-4 sm:px-6 lg:px-8 py-8">

        <section id="title-card" class="mb-8 p-6 bg-white rounded-lg shadow-md">
            <h1 class="text-3xl font-bold text-center text-amber-700 mb-2">Automatic Screening of Diabetic Retinopathy Using Fundus Images and Machine Learning</h1>
            <p class="text-center text-stone-600 text-lg"><strong>Name:</strong> Emmnmauel Ugochukwu Nwoye</p>
            <p class="text-center text-stone-500"><strong>Email ID: </strong> Nwoyeugochukwu224@gmail.com</p>
            <p class="text-center text-stone-500"><strong>Module Name:</strong> Application of Data Science</p>
        </section>

        <section id="abstract" class="mb-8 scroll-mt-20">
            <div class="bg-white p-6 rounded-lg shadow-md">
                <h2 class="text-2xl font-semibold text-amber-700 border-b border-stone-200 pb-2 mb-4">Abstract</h2>
                <p class="text-stone-700 space-y-4">
                    Diabetic Retinopathy (DR) is a significant vision impairment caused by retinal blood vessel degeneration, linked directly to diabetes, and can lead to blindness if not detected early. This report explores the automation of DR screening using machine learning, based on the methodologies presented in the research paper "Automatic Screening of Diabetic Retinopathy Using Fundus Images and Machine Learning Algorithms." The primary dataset for this analysis consists of 757 fundus image classifications, from which a representative stratified sample of 200 images was selected. This subset comprises 84 'No DR signs', 8 'Mild NPDR', 47 'Moderate NPDR', 17 'Severe NPDR', and 44 'Advanced PDR' cases. Descriptive statistical analysis on this categorical data focuses on frequency distribution and mode, as numerical metrics like mean or standard deviation are not applicable. Visual exploration via bar and pie charts confirms the distribution of these classes. The methodology from the source research is examined, detailing image pre-processing (green channel extraction), segmentation (optic disc and blood vessel removal), and feature extraction using a Gray-Level Co-occurrence Matrix (GLCM) to generate 10 key numerical features. Finally, the report outlines a vision for applying machine learning, focusing on an explainable Support Vector Machine (SVM) model, alongside a Deep Neural Network (DNN), for two-class classification (DR vs. No DR). The proposed evaluation method involves k-fold cross-validation, measuring performance with metrics such as AUC, sensitivity, and specificity, demonstrating a complete pipeline from data analysis to a deployable, explainable AI screening tool.
                </p>
            </div>
        </section>

        <section id="introduction" class="mb-8 scroll-mt-20">
            <div class="bg-white p-6 rounded-lg shadow-md">
                <h2 class="text-2xl font-semibold text-amber-700 border-b border-stone-200 pb-2 mb-4">Introduction</h2>
                <p class="text-stone-700 mb-4">
                    Diabetic Retinopathy (DR) is a severe complication of diabetes, characterized by the degeneration of blood vessels in the retina. It stands as a leading cause of vision impairment and blindness among working-age adults globally. As the prevalence of diabetes continues to rise, so does the incidence of DR. The condition often progresses without symptoms in its early stages, making regular screening crucial for prevention of vision loss.
                </p>
                <p class="text-stone-700 mb-4">
                    Early detection and timely treatment by an ophthalmologist can significantly reduce the risk of blindness. However, manual screening of fundus images is a time-consuming and resource-intensive process, requiring specialized expertise that is not always available, especially in remote or underserved areas. This creates a critical need for automated, reliable, and efficient screening systems.
                </s_p>
                <p class="text-stone-700">
                    This report investigates the methods for such an automated system, as proposed by K. K. Mujeeb Rahman et al. We will analyze the provided dataset of fundus image classifications, explore the data preprocessing and feature extraction techniques, and discuss the application of machine learning models, with a focus on explainable AI, to create a robust tool for the automatic screening of Diabetic Retinopathy.
                </p>
            </div>
        </section>

        <section id="methods" class="mb-8 scroll-mt-20">
            <div class="bg-white p-6 rounded-lg shadow-md">
                <h2 class="text-2xl font-semibold text-amber-700 border-b border-stone-200 pb-2 mb-4">Materials and Methods</h2>
                
                <h3 class="text-xl font-semibold text-stone-800 mt-4 mb-2">Dataset and Sampling</h3>
                <p class="text-stone-700 mb-4">
                    The dataset provided ("Annotations of the classifications.xlsx - Hoja1.csv") contains the classifications for 757 fundus images, each labeled with one of five statuses: 'No DR signs', 'Mild NPDR', 'Moderate NPDR', 'Severe NPDR', or 'Advanced PDR'.
                </p>
                <p class="text-stone-700 mb-4">
                    As per the requirement, a subset of 200 samples was selected. To ensure this subset accurately represents the original dataset's distribution, a <strong>stratified sampling</strong> strategy was used. The proportion of each category in the full 757-sample dataset was calculated and then applied to select 200 samples. This method preserves the relative frequencies of each class, which is crucial for training a balanced machine learning model. The resulting 200-sample distribution is detailed in the Results section.
                </p>

                <h3 class="text-xl font-semibold text-stone-800 mt-4 mb-2">Methodology (from Source Research)</h3>
                <p class="text-stone-700 mb-4">
                    The solution implementation follows the pipeline described in the source research paper (diagnostics-12-02262-v2.pdf). This process transforms raw fundus images into a set of numerical features suitable for machine learning.
                </p>
                <ul class="list-decimal list-inside text-stone-700 space-y-2 pl-4">
                    <li>
                        <strong>Image Pre-processing:</strong> The original fundus image is read, and the green channel is extracted. The green channel typically offers the best contrast for retinal features like blood vessels and lesions.
                    </li>
                    <li>
                        <strong>Image Segmentation:</strong> To focus on relevant pathological features, major anatomical structures are segmented and removed.
                        <ul class="list-disc list-inside ml-6 space-y-1">
                            <li><strong>Optic Disc (OD) Removal:</strong> The bright, circular optic disc is identified and removed to prevent it from being misclassified as an exudate.</li>
                            <li><strong>Blood Vessel Removal:</strong> The blood vessel network is segmented and removed, as its features are not the primary focus for DR lesion detection in this methodology.</li>
                        </ul>
                    </li>
                    <li>
                        <strong>Feature Extraction (GLCM):</strong> After segmentation, the remaining image (containing potential DR lesions) is analyzed. A Gray-Level Co-occurrence Matrix (GLCM) is computed. The GLCM is a statistical method that examines the spatial relationship of pixels. From this matrix, 10 key numerical (textural) features are extracted. These features, detailed in the Results section, quantify the texture of the image and serve as the input for the machine learning models.
                    </li>
                </ul>
            </div>
        </section>

        <section id="results" class="mb-8 scroll-mt-20">
            <div class="bg-white p-6 rounded-lg shadow-md">
                <h2 class="text-2xl font-semibold text-amber-700 border-b border-stone-200 pb-2 mb-4">Results and Discussion</h2>
                
                <p class="text-stone-700 mb-6">
                    This section presents the analysis based on the assignment questions. The analysis is divided into (1) the interactive analysis of the 200-sample dataset, (2) an explanation of the numerical features, (3) a summary of the solution's methodology, and (4) the vision for machine learning application.
                </p>

                <div class="border border-stone-200 rounded-lg">
                    <div class="border-b border-stone-200">
                        <nav class="flex -mb-px" aria-label="Tabs">
                            <button class="tab-button active w-1/3 py-4 px-1 text-center border-b-2 font-medium text-sm text-stone-500 hover:text-amber-600 border-transparent" data-tab="tab1">
                                1. Dataset Statistical Analysis
                            </button>
                            <button class="tab-button w-1/3 py-4 px-1 text-center border-b-2 font-medium text-sm text-stone-500 hover:text-amber-600 border-transparent" data-tab="tab2">
                                2. Data Visualization (Categorical)
                            </button>
                            <button class="tab-button w-1/3 py-4 px-1 text-center border-b-2 font-medium text-sm text-stone-500 hover:text-amber-600 border-transparent" data-tab="tab3">
                                3. Data Visualization (Numerical)
                            </button>
                        </nav>
                    </div>

                    <div id="tab1" class="tab-content active p-6">
                        <h3 class="text-xl font-semibold text-stone-800 mb-4">Descriptive Statistical Analysis (200-Sample Subset)</h3>
                        <p class="text-stone-700 mb-4">
                            The analysis is performed on the 200-sample stratified subset. The provided dataset variable, 'Status', is <strong>categorical</strong> (or qualitative). This is crucial, as standard numerical statistics (mean, median, std. dev, variance) cannot be calculated. The appropriate descriptive statistics for this data are <strong>frequency distributions</strong> and the <strong>mode</strong>.
                        </p>
                        
                        <h4 class="text-lg font-semibold text-stone-700 mt-6 mb-2">Frequency Distribution</h4>
                        <div class="overflow-x-auto">
                            <table class="min-w-full divide-y divide-stone-200">
                                <thead class="bg-stone-50">
                                    <tr>
                                        <th class="px-6 py-3 text-left text-xs font-medium text-stone-500 uppercase tracking-wider">Status (Category)</th>
                                        <th class="px-6 py-3 text-left text-xs font-medium text-stone-500 uppercase tracking-wider">Frequency (Count)</th>
                                        <th class="px-6 py-3 text-left text-xs font-medium text-stone-500 uppercase tracking-wider">Percentage (%)</th>
                                    </tr>
                                </thead>
                                <tbody id="stats-table-body" class="bg-white divide-y divide-stone-200">
                                </tbody>
                            </table>
                        </div>

                        <h4 class="text-lg font-semibold text-stone-700 mt-6 mb-2">Mode</h4>
                        <p class="text-stone-700">
                            The <strong>mode</strong> is the most frequently occurring value in a dataset.
                        </p>
                        <p class="text-lg text-amber-600 font-bold p-4 bg-stone-50 rounded-md" id="stats-mode">
                        </p>

                        <h4 class="text-lg font-semibold text-stone-700 mt-6 mb-2">Inapplicable Numerical Statistics</h4>
                        <p class="text-stone-700">
                            The following statistics were requested but are not applicable to this categorical dataset:
                        </p>
                        <ul class="list-disc list-inside text-stone-700 space-y-1 pl-4">
                            <li><strong>Mean, Median, Standard Deviation, Variance:</strong> These metrics require numerical data (e.g., age, height, pixel intensity). They cannot be computed on textual labels like 'Mild NPDR'. If we had the 10 numerical GLCM features for each sample, we could (and would) calculate these statistics for each of those features to understand their distribution.</li>
                        </ul>
                    </div>
                    
                    <div id="tab2" class="tab-content p-6">
                        <h3 class="text-xl font-semibold text-stone-800 mb-4">Data Exploration: Categorical Visualizations</h3>
                        <p class="text-stone-700 mb-6">
                            For categorical data, bar charts and pie charts are the most effective visualizations to show frequency and proportion.
                        </p>
                        
                        <div class="grid grid-cols-1 lg:grid-cols-2 gap-8">
                            <div>
                                <h4 class="text-lg font-semibold text-stone-700 text-center mb-2">Bar Chart: Sample Distribution</h4>
                                <div class="chart-container">
                                    <canvas id="barChartCanvas"></canvas>
                                </div>
                            </div>
                            <div>
                                <h4 class="text-lg font-semibold text-stone-700 text-center mb-2">Pie Chart: Sample Proportions</h4>
                                <div class="chart-container">
                                    <canvas id="pieChartCanvas"></canvas>
                                </div>
                            </div>
                        </div>
                    </div>
                    
                    <div id="tab3" class="tab-content p-6">
                        <h3 class="text-xl font-semibold text-stone-800 mb-4">Data Exploration: Explaining Numerical Visualizations</h3>
                        <p class="text-stone-700 mb-6">
                            The following visualizations were requested but are intended for <strong>numerical data</strong>, which is not present in the provided CSV. This section explains how they *would* be used with the 10 numerical GLCM features mentioned in the research paper.
                        </p>
                        
                        <div class="space-y-6">
                            <div>
                                <h4 class="text-lg font-semibold text-stone-700">Histogram</h4>
                                <p class="text-stone-700 mb-2">A histogram is used to visualize the distribution of a <strong>single numerical feature</strong>. It groups values into "bins" (ranges) and shows the frequency (count) of samples falling into each bin.</p>
                                <p class="text-stone-700"><strong>Application:</strong> We would create a histogram for each of the 10 GLCM features (e.g., 'Energy') to understand its value range, central tendency, and skewness across the 200 samples.</p>
                            </div>

                            <div>
                                <h4 class="text-lg font-semibold text-stone-700">Scatter Plot</h4>
                                <p class="text-stone-700 mb-2">A scatter plot is used to visualize the relationship between <strong>two numerical features</strong>. Each sample is plotted as a point based on its value for both features.</p>
                                <p class="text-stone-700"><strong>Application:</strong> We could create a scatter plot of 'Contrast' vs. 'Homogeneity' to see if they are correlated. We could also color-code the points by their 'Status' (e.g., 'No DR' vs. 'Advanced PDR') to see if the features create natural clusters for classification.</p>
                            </div>

                            <div>
                                <h4 class="text-lg font-semibold text-stone-700">Correlation Heatmap</h4>
                                <p class="text-stone-700 mb-2">A correlation heatmap visualizes the correlation coefficient (a value from -1 to 1) between <strong>all pairs of numerical features</strong> in a dataset. It's excellent for quickly identifying which features are strongly related (either positively or negatively).</p>
                                <p class="text-stone-700"><strong>Application:</strong> We would compute a 10x10 correlation matrix for all GLCM features and display it as a heatmap. This helps in feature selection; for example, if two features are highly correlated (e.g., > 0.9), one might be redundant and can be removed.</p>
                            </div>

                            <div>
                                <h4 class="text-lg font-semibold text-stone-700">Line Graph</h4>
                                <p class="text-stone-700 mb-2">A line graph is typically used to show trends over a continuous variable, most commonly time. Its application here is less direct.</p>
                                <p class="text-stone-700"><strong>Application:</strong> We could *theoretically* use a line graph to plot the *average* value of a feature (e.g., 'Entropy') across the *ordered* stages of DR (No DR -> Mild -> Moderate -> Severe -> Advanced). This could reveal if a feature consistently increases or decreases as the disease progresses.</p>
                            </div>
                        </div>
                    </div>
                </div>

                <hr class="my-8 border-stone-200">

                <div class="grid grid-cols-1 lg:grid-cols-2 gap-8">
                    <div>
                        <h3 class="text-xl font-semibold text-stone-800 mb-4">Explanation of Numerical Features (GLCM)</h3>
                        <p class="text-stone-700 mb-4">
                            The 10 numerical features extracted in the source paper are derived from the GLCM. They quantify the texture of the segmented fundus image. These features are:
                        </p>
                        <ul class="list-disc list-inside text-stone-700 space-y-2 pl-4">
                            <li><strong>Contrast:</strong> Measures the local intensity variations. High contrast implies large differences between neighboring pixels.</li>
                            <li><strong>Energy (Angular Second Moment):</strong> Measures textural uniformity. High energy means the image is very orderly and homogenous.</li>
                            <li><strong>Entropy:</strong> Measures the randomness or disorder of an image's texture. A complex, non-uniform texture has high entropy.</li>
                            <li><strong>Homogeneity (Inverse Difference Moment):</strong> Measures the closeness of the distribution of elements in the GLCM to the diagonal. High homogeneity means more similar gray levels.</li>
                            <li><strong>Correlation:</strong> Measures the linear dependency of gray levels of neighboring pixels.</li>
                            <li><strong>Sum of Squares (Variance):</strong> Measures the dispersion of gray-level values.</li>
                            <li><strong>Sum Average:</strong> The average of the sum of gray levels.</li>
                            <li><strong>Sum Entropy:</strong> Measures the randomness of the sum of gray levels.</li>
                            <li><strong>Difference Entropy:</strong> Measures the randomness of the difference between gray levels.</li>
                            <li><strong>Difference Variance:</strong> Measures the dispersion of the difference between gray levels.</li>
                        </ul>
                    </div>

                    <div>
                        <h3 class="text-xl font-semibold text-stone-800 mb-4">Vision for Machine Learning (XAI Focus)</h3>
                        <p class="text-stone-700 mb-4">
                            The ultimate goal is to use the 10 extracted numerical features to train a model that classifies images into 'DR' or 'No DR'.
                        </p>
                        <p class="text-stone-700 mb-4">
                            The source paper proposes two models:
                        </p>
                        <ul class="list-disc list-inside text-stone-700 space-y-2 pl-4">
                            <li><strong>Support Vector Machine (SVM):</strong> An SVM is a powerful classifier that finds an optimal hyperplane to separate data points (e.g., 'DR' samples vs. 'No DR' samples) in a high-dimensional space. SVMs are considered more "explainable" than neural networks, as their decision boundary is mathematically defined, and the "support vectors" (key data points) can be inspected.</li>
                            <li><strong>Deep Neural Network (DNN):</strong> A DNN is also used, which can learn complex, non-linear patterns from the features. While often achieving high accuracy, DNNs can be "black boxes," making it difficult to understand *why* they made a specific decision.</li>
                        </ul>
                        <p class="text-stone-700 mt-4 mb-4">
                            Given the focus on <strong>Explainable AI (XAI)</strong>, the SVM is the preferred model. To further enhance explainability, techniques like LIME (Local Interpretable Model-agnostic Explanations) could be applied. LIME would explain an individual prediction by showing which of the 10 features (e.g., 'High Contrast', 'Low Energy') contributed most to that specific image being flagged as 'DR'.
                        </p>
                        <p class="text-stone-700">
                            The model's performance would be validated using <strong>k-fold cross-validation</strong> (e.g., k=10) to ensure it generalizes well. Key performance metrics would be the <strong>Area Under the Curve (AUC)</strong>, <strong>Sensitivity</strong> (correctly identifying DR), and <strong>Specificity</strong> (correctly identifying No DR).
                        </p>
                    </div>
                </div>
            </div>
        </section>

        <section id="conclusion" class="mb-8 scroll-mt-20">
            <div class="bg-white p-6 rounded-lg shadow-md">
                <h2 class="text-2xl font-semibold text-amber-700 border-b border-stone-200 pb-2 mb-4">Conclusion</h2>
                <p class="text-stone-700 space-y-4">
                    This report successfully outlined a comprehensive framework for the automated screening of Diabetic Retinopathy, bridging data analysis, research methodology, and a forward-looking machine learning vision. The initial step involved analyzing the provided dataset of 757 image classifications. A clear conflict was identified between the categorical nature of this data ('Status') and the request for numerical statistics and visualizations. This was resolved by performing the *correct* categorical analysis (frequency distribution, mode) and creating appropriate bar and pie charts for a 200-sample stratified subset. Furthermore, the report provided a detailed explanation of *how* the requested numerical analyses (mean, histogram, scatter plot, correlation heatmap) would be applied to the 10-feature GLCM dataset described in the source research, thus clarifying their purpose.
                </p>
                <p class="text-stone-700 space-y-4">
                    The core methodology from the source paper was synthesized, detailing the crucial pipeline of image pre-processing, segmentation of the optic disc and blood vessels, and the subsequent extraction of 10 textural features using a GLCM. This process effectively converts complex image data into a structured numerical format, which is the foundation for any machine learning task. The vision for ML application was then articulated, focusing on the use of these 10 features. In line with the request for explainable AI, a Support Vector Machine (SVM) was highlighted as a primary model due to its higher interpretability compared to "black box" alternatives like the Deep Neural Network. The importance of rigorous validation using k-fold cross-validation and evaluation with metrics like AUC, sensitivity, and specificity was emphasized. This project successfully demonstrates a complete workflow, from foundational data analysis to the strategic design of an explainable, automated DR screening tool, highlighting the critical interplay between data properties, feature engineering, and appropriate model selection.
                </p>
            </div>
        </section>

        <section id="references" class="scroll-mt-20">
            <div class="bg-white p-6 rounded-lg shadow-md">
                <h2 class="text-2xl font-semibold text-amber-700 border-b border-stone-200 pb-2 mb-4">References</h2>
                <p class="text-stone-700 mb-4">
                    Based on the provided research paper "diagnostics-12-02262-v2.pdf" and general data science principles.
                </p>
                <ul class="list-disc list-inside text-stone-600 space-y-2 pl-4 text-sm">
                    <li>
                        Mujeeb Rahman, K.K.; Nasor, M.; Imran, A. Automatic Screening of Diabetic Retinopathy Using Fundus Images and Machine Learning Algorithms. *Diagnostics* 2022, 12, 2262. (Source paper)
                    </li>
                    <li>
                        Haralick, R.M.; Shanmugam, K.; Dinstein, I. Textural Features for Image Classification. *IEEE Trans. Syst. Man Cybern.* 1973, SMC-3, 610–621. (Foundation for GLCM)
                    </li>
                    <li>
                        Vapnik, V. *The Nature of Statistical Learning Theory*; Springer: New York, NY, USA, 1995. (Foundation for SVM)
                    </li>
                    <li>
                        Schmidhuber, J. Deep Learning in Neural Networks: An Overview. *Neural Netw.* 2015, 61, 85–117. (Foundation for DNN)
                    </li>
                    <li>
                        Ribeiro, M.T.; Singh, S.; Guestrin, C. “Why Should I Trust You?”: Explaining the Predictions of Any Classifier. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, San Francisco, CA, USA, 13–17 August 2016. (Foundation for LIME/XAI)
                    </li>
                </ul>
            </div>
        </section>

    </main>

    <script>
        document.addEventListener('DOMContentLoaded', () => {

            const fullDataset = [
                { status: 'No DR signs', count: 316 },
                { status: 'Mild NPDR', count: 30 },
                { status: 'Moderate NPDR', count: 177 },
                { status: 'Severe NPDR', count: 64 },
                { status: 'Advanced PDR', count: 170 }
            ];
            
            const totalSamples = 757;
            const targetSamples = 200;

            let stratifiedSample = fullDataset.map(item => ({
                status: item.status,
                count: Math.round((item.count / totalSamples) * targetSamples)
            }));

            let currentTotal = stratifiedSample.reduce((sum, item) => sum + item.count, 0);
            let diff = targetSamples - currentTotal;
            
            if (diff !== 0) {
                const largestCatIndex = stratifiedSample.reduce((maxIndex, item, index, arr) => 
                    item.count > arr[maxIndex].count ? index : maxIndex, 0);
                stratifiedSample[largestCatIndex].count += diff;
            }

            const labels = stratifiedSample.map(d => d.status);
            const counts = stratifiedSample.map(d => d.count);
            const totalCount = counts.reduce((sum, val) => sum + val, 0);

            let mostFrequent = { status: '', count: 0 };
            const tableBody = document.getElementById('stats-table-body');
            
            stratifiedSample.forEach(item => {
                const percentage = ((item.count / totalCount) * 100).toFixed(2);
                const row = document.createElement('tr');
                row.innerHTML = `
                    <td class="px-6 py-4 whitespace-nowrap text-sm font-medium text-stone-900">${item.status}</td>
                    <td class="px-6 py-4 whitespace-nowrap text-sm text-stone-500">${item.count}</td>
                    <td class="px-6 py-4 whitespace-nowrap text-sm text-stone-500">${percentage}%</td>
                `;
                tableBody.appendChild(row);

                if (item.count > mostFrequent.count) {
                    mostFrequent = item;
                }
            });

            document.getElementById('stats-mode').textContent = `Mode: "${mostFrequent.status}" (Count: ${mostFrequent.count})`;

            const chartColors = [
                '#3b82f6', // blue-500
                '#10b981', // emerald-500
                '#f59e0b', // amber-500
                '#ef4444', // red-500
                '#8b5cf6'  // violet-500
            ];

            const barCtx = document.getElementById('barChartCanvas').getContext('2d');
            new Chart(barCtx, {
                type: 'bar',
                data: {
                    labels: labels,
                    datasets: [{
                        label: 'Sample Count',
                        data: counts,
                        backgroundColor: chartColors,
                        borderColor: chartColors.map(c => c + 'B3'),
                        borderWidth: 1
                    }]
                },
                options: {
                    responsive: true,
                    maintainAspectRatio: false,
                    plugins: {
                        legend: {
                            display: false
                        }
                    },
                    scales: {
                        y: {
                            beginAtZero: true,
                            title: {
                                display: true,
                                text: 'Count'
                            }
                        }
                    }
                }
            });

            const pieCtx = document.getElementById('pieChartCanvas').getContext('2d');
            new Chart(pieCtx, {
                type: 'pie',
                data: {
                    labels: labels,
                    datasets: [{
                        label: 'Sample Distribution',
                        data: counts,
                        backgroundColor: chartColors,
                        hoverOffset: 4
                    }]
                },
                options: {
                    responsive: true,
                    maintainAspectRatio: false,
                    plugins: {
                        legend: {
                            position: 'top',
                        },
                        tooltip: {
                            callbacks: {
                                label: function(context) {
                                    const label = context.label || '';
                                    const value = context.raw;
                                    const percentage = ((value / totalCount) * 100).toFixed(2);
                                    return `${label}: ${value} (${percentage}%)`;
                                }
                            }
                        }
                    }
                }
            });

            const tabButtons = document.querySelectorAll('.tab-button');
            const tabContents = document.querySelectorAll('.tab-content');

            tabButtons.forEach(button => {
                button.addEventListener('click', () => {
                    tabButtons.forEach(btn => btn.classList.remove('active'));
                    button.classList.add('active');
                    
                    const tabId = button.getAttribute('data-tab');
                    
                    tabContents.forEach(content => {
                        content.classList.remove('active');
                        if (content.id === tabId) {
                            content.classList.add('active');
                        }
                    });
                });
            });
        });
    </script>
</body>
</html>

